---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.13.8
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import calendar
import datetime
import json 
import os  
import pickle
from configparser import ConfigParser
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.preprocessing import OneHotEncoder
```

### General helper functions

```{python}
# format release date
def format_release_date(data):
    arr = list()
    for i,l in enumerate(data):
        if ord(l) == 45:
            arr.append(i)
    date = int(data[0:arr[0]])
    month_name = data[arr[0]+1:arr[1]]
    datetime_object = datetime.datetime.strptime(month_name, "%b")
    month = datetime_object.month
    year = int(data[arr[1]+1:arr[1]+5])
    date = datetime.datetime(year, month, date)
    utc_time = calendar.timegm(date.utctimetuple())
    return utc_time 

# format timestamp
def date_only_from_datetime(data):
    temp = datetime.datetime.fromtimestamp(data)
    date = datetime.datetime(temp.year, temp.month, temp.day) # temp.hour
    return date

# get mean rating for the user x
def get_mean_rating(x, mean_rating):
    return float(mean_rating[mean_rating['user_id'] == x]['mean_rating'].to_numpy()[0])
```

```{python}
#Read config.ini file
config = ConfigParser()
config.read("config.ini")
fileinfo = config["DATASETS"]

path = fileinfo['ori_path']
fm_export_path = fileinfo['fm_path']
rf_export_path = fileinfo['rf_path']
train_set_filename = fileinfo['train_filename']
test_set_filename = fileinfo['test_filename']
description_filename = fileinfo['description_filename']

# import user, data, item and genre data
u_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']
users = pd.read_csv(path + 'ml-100k/u.user', sep='|', names=u_cols, encoding='latin-1', parse_dates=True, header=None) 
d_cols = ['user_id', 'movie_id', 'rating', 'timestamp']
data = pd.read_csv(path + 'ml-100k/u.data', sep='\t', names=d_cols, encoding='latin-1', header=None)
genre_cols = pd.read_csv(path + 'ml-100k/u.genre', sep='|', header=None)[0].to_numpy().tolist()
m_cols = ['movie_id', 'movie_title', 'release_date', 'video_release_date','imdb_url']
m_cols.extend(genre_cols)
movies = pd.read_csv(path + 'ml-100k/u.item', sep='|', names=m_cols, usecols=range(24), encoding='latin-1', header=None)

# merge user, data and item
movie_ratings = pd.merge(movies, data)
df = pd.merge(movie_ratings, users)
```

### Before split - general


- These are columns that need to be preprocessed for both FM and RF, but are don't need to be split beforehand
- Columns: 
  - release_date
  - sex
  - zip_code

```{python}
# dropping some rows that has no release date 
df.drop(index = df[df['release_date'].isnull() == True].index, inplace = True)
df.reset_index(drop=True, inplace=True)

# format release_date to unix timestamp
df['release_date'] = df['release_date'].apply(lambda x: format_release_date(x))

# convert to binary
df['sex'] = (df['sex'] == 'M').astype(int)

# take first character only
df['zip_code'] = df['zip_code'].str[0]
```

### Before split - columns for FM


- These are columns that need preprocessing specific for FM, but don't have to be split beforehand
- Columns:
  - age
  - timestamp
  - release_date
- In addition, the one-hot-encoders are fitted here, before splitting

```{python}
# format age
age_labels = []
age_bins = np.arange(0,90,10)
for i in range(len(age_bins)-1):
    age_labels.append(str(age_bins[i]) + '-' + str(age_bins[i+1]))
# convert ages to group of age ranges
df['fm_age'] = pd.cut(df['age'], bins = age_bins, labels=age_labels)
```

```{python}
# preprocess timestamp and release date
# only use year, month and day from the datetime
df['fm_timestamp'] = df['timestamp'].apply(lambda x : date_only_from_datetime(x))
df['fm_release_date'] = df['release_date']
# df['fm_release_date'] = df['release_date'].apply(lambda x : date_only_from_datetime(x))
```

```{python}
# fit one-hot-encoders before train/test split
ohe_columns = ["movie_id", "user_id", "fm_timestamp", "fm_release_date", "zip_code", "fm_age", "occupation", "movie_id"]
ohe_encoders = []
for col in ohe_columns:
    encoder = OneHotEncoder(dtype=int, sparse=False)
    encoder.fit(df[[col]])
    ohe_encoders.append(encoder)
```

### Before split - columns for RF


- These are columns that need preprocessing specific for RF, but don't need to be split beforehand
- Columns: none at the moment.


### Train/Test split

```{python}
# Splitting the data into test and train
X_train, X_test, y_train, y_test = train_test_split(df.drop(['rating'], axis = 1), df['rating'], random_state=42)

# training and test data is combined to a single dataframe
df_train = pd.concat([X_train,pd.DataFrame(y_train)], axis = 1)
df_test = pd.concat([X_test,pd.DataFrame(y_test)], axis = 1)

del(df)
```

### After split - general


- These are columns that have to be preprocessed after the split.
- Columns:
  - mean_rate
  - cum_mean_rate
  - last_watched

```{python}
# get last watched movie
def get_last_watched(df):
    df.sort_values(['user_id', 'timestamp'], inplace=True)
    # shift the values in movie_id col down by one
    df['last_watched'] = df['movie_id'].shift(1)
    # then set the first value (first movie watched by the user) to 0
    for index, user in users.iterrows():
        idx = df[df['user_id'] == user['user_id']].index[0]
        df.at[idx, 'last_watched'] = df.at[idx, 'movie_id']
```

```{python}
for df in [df_train, df_test]:
    get_last_watched(df)
```

```{python}
# Get mean and cumulative mean
# it is only done for training data, since we should not take statistics about rating from test data
data_train = df_train[['user_id','movie_id','rating','timestamp']]

# calculating cumulative mean rating based on timestamp sorted in ascending order
data_train = data_train.sort_values(['user_id', 'timestamp']).reset_index(drop=True)
data_train['one'] = 1
data_train['cumsum'] = data_train.groupby('user_id')['one'].cumsum()
data_train['cum_mean_rate'] = data_train.groupby('user_id')['rating'].cumsum() / data_train['cumsum']

# calculating mean rating for each user
mean_rating = data_train.groupby('user_id')['rating'].sum() / data_train.groupby('user_id')['one'].sum()
mean_rating = mean_rating.reset_index()
mean_rating.rename(columns = {0:'mean_rating'}, inplace = True)

# getting mean rate for the corresponding user
data_train['mean_rate'] = data_train['user_id']
data_train["mean_rate"] = data_train['mean_rate'].apply(lambda x: get_mean_rating(x, mean_rating))

# merging to the original dataframe
df_train = df_train.merge(data_train, how='left', left_on=['timestamp', 'user_id', 'movie_id', 'rating'], right_on=['timestamp', 'user_id', 'movie_id', 'rating'])

# during testing we won't have the rating value, so we should use the running mean form training data.
df_test['mean_rate'] = df_test['user_id'].apply(lambda x: get_mean_rating(x, mean_rating))
df_test['cum_mean_rate'] = df_test['mean_rate']

del(data_train)
```

```{python}
# RF forest data needs to be preprocessed differently. 
# Therefore need to save copy of dfs first
rf_df_train = df_train.copy(deep=True)
rf_df_test = df_test.copy(deep=True)
```

### After split - FM


- These are steps that need to be performed after the split, and specifically for FM
- Steps:
  - OHE and other encoding
  - Getting the feature column lengths (col_len)
  - Saving the data
- columns used in FM:
  - movie_id, user_id, timestamp, release_date, zip_code, age, occupation, genre, sex, rating
  - TODO: binary encoding last_watched, maybe?

```{python}
ohe_columns[-1] = "last_watched"
ohe_encoders[-1].feature_names_in_ = [ohe_columns[-1]]
```

```{python}
# add mean_rate and cum_mean_rate to encoder
for col in ['mean_rate', 'cum_mean_rate']:
    df_train[col] = df_train[col].apply(lambda rate: int(round(rate, 1)*10))
    df_test[col] = df_test[col].apply(lambda rate: int(round(rate, 1)*10))
    ohe_columns.append(col)
    encoder = OneHotEncoder(categories=[[i for i in range(10,51,1)]], dtype=int, sparse=False)
    encoder.fit(df_train[[col]])
    ohe_encoders.append(encoder)
```

```{python}
# create the final df and set col_len at the same time
def encode_df(df, col_len, ohe_cols, ohe_encoders):
    # create empty df with correct shape
    df_final = pd.DataFrame(columns = [], index = range(len(df)))
    
    # columns that need ohe
    for index, col in enumerate(ohe_cols):
        transform = pd.DataFrame(ohe_encoders[index].transform(df[[col]]), columns=ohe_encoders[index].get_feature_names_out())
        df_final = pd.concat([df_final, transform], axis = 1)
        col_len[col] = len(transform.columns)

    # special case: genre (already ohe in original dataset)
    col_len["genre"] = len(genre_cols)
    df_final = pd.concat([df_final, df[genre_cols]], axis = 1)

    # columns that don't need ohe
    non_ohe_columns = ["sex"]
    for col in non_ohe_columns:
        col_len[col] = 1
    df_final = pd.concat([df_final, df[non_ohe_columns]], axis = 1)

    # rating column (no need to add to col_len)
    df_final = pd.concat([df_final, df[['rating']]], axis = 1)

    return df_final
```

```{python}
col_len = {}

#need to reset otherwise pd.concat will not work properly
df_test.reset_index(inplace=True)
df_train.reset_index(inplace=True)

df_test = encode_df(df_test, col_len, ohe_columns, ohe_encoders)
df_train = encode_df(df_train, col_len, ohe_columns, ohe_encoders)
```

```{python}
# Saving the FM data
# create parent folder if doesn't exist
os.makedirs(fm_export_path, exist_ok=True)

# export csv file
pickle.dump(df_train, open(fm_export_path + train_set_filename, 'wb'))
pickle.dump(df_test, open(fm_export_path + test_set_filename, 'wb'))

# saving info about data
with open(fm_export_path + description_filename, "w") as outfile:
    json.dump(col_len, outfile)

```

### After split - RF


- These are steps that need to be performed after the split, and specifically for RF
- Steps:
  - label encoding
  - Saving the data
- Columns used in RF:
  - movie_id, user_id, timestamp, release_date, zip_code, age, 
  occupation, sex, rating, last_watched, mean_rate, cum_mean_rate

```{python}
final_columns = ['movie_id', 'user_id', "timestamp", "release_date", "zip_code", "age", 
"occupation", "sex", "rating", "last_watched", "mean_rate", "cum_mean_rate"]
final_columns.extend(genre_cols)
# droping all unnecessary columns
df_train = rf_df_train[final_columns]
df_test = rf_df_test[final_columns]

del(rf_df_train)
del(rf_df_test)
```

```{python}
# converting objects to integers, since scikit learn supports only numerical values
le = preprocessing.LabelEncoder()

label_encoding_columns = ['occupation','zip_code','sex']
for col in label_encoding_columns:
    df_train[col] = le.fit_transform(df_train[col])
    df_test[col] = le.fit_transform(df_test[col])    
```

```{python}
# create parent folder if doesn't exist
os.makedirs(rf_export_path, exist_ok=True)

# Saving the RF data
pickle.dump(df_train, open(rf_export_path + train_set_filename, 'wb'))
pickle.dump(df_test, open(rf_export_path + test_set_filename, 'wb'))

```

```{python}
df_train.columns
```

```{python}
df_train.shape
```

### Feature selection with Mutual Information Regression method

```{python}
from sklearn.feature_selection import mutual_info_regression

X_train = df_train.drop((['rating']), axis = 1)
y_train = df_train['rating']

mutual_info = mutual_info_regression(X_train, y_train)
mutual_info
```

```{python}
mutual_info = pd.Series(mutual_info)
mutual_info.index = X_train.columns
mutual_info.sort_values(ascending=False)
```

```{python}
mutual_info.sort_values(ascending=False).plot.bar(figsize=(15,5))
```

### Feature selection with F regression method

```{python}
# example of correlation feature selection for numerical data
from sklearn.datasets import make_regression
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression

# feature selection
def select_features(X_train, y_train, X_test):
    # configure to select all features
    fs = SelectKBest(score_func=f_regression, k='all')
    # learn relationship from training data
    fs.fit(X_train, y_train)
    # transform train input data
    X_train_fs = fs.transform(X_train)
    # transform test input data
    X_test_fs = fs.transform(X_test)
    return X_train_fs, X_test_fs, fs
 
# split into train and test sets
X_test = df_test.drop((['rating']), axis = 1)
y_test = df_test['rating']

# feature selection
X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)

# what are scores for the features
for i in range(len(fs.scores_)):
    print('Feature %d: %f' % (i, fs.scores_[i]))
    
# plot the scores
plt.bar([i for i in range(len(fs.scores_))], fs.scores_)
plt.show()
```

```{python}
df_fs = pd.Series(fs.scores_)
df_fs.sort_values(ascending=False).plot.bar(figsize=(15,5))
```

```{python}
print(X_train.columns[10])
print(X_train.columns[9])
print(X_train.columns[0])
print(X_train.columns[8])
print(X_train.columns[19])
print(X_train.columns[28])
print(X_train.columns[16])
print(X_train.columns[5])
print(X_train.columns[22])
print(X_train.columns[21])
```

### Feature selection with Sequential Feature Selection method

```{python}
from sklearn.feature_selection import SequentialFeatureSelector as SFS
from sklearn.neighbors import KNeighborsClassifier

X_train = df_train.drop((['rating']), axis = 1)
y_train = df_train['rating']

model = KNeighborsClassifier(n_neighbors=10)

# Using Forward Elimination
sfs = SFS(model, 
         n_features_to_select = 10,
         direction = 'forward',
         scoring = 'accuracy',
         n_jobs = -1,
         cv=10)

sfs = sfs.fit(X_train, y_train)
```

```{python}
X_train.shape[1]
```

```{python}
X_train.columns
```

```{python}
#X_train.T
```

```{python}
np.arange(X_train.shape[1])[sfs.support_]
```

```{python}
print(X_train.columns[0])
print(X_train.columns[9])
print(X_train.columns[10])
print(X_train.columns[11])
print(X_train.columns[13])
print(X_train.columns[14])
print(X_train.columns[15])
print(X_train.columns[17])
print(X_train.columns[21])
print(X_train.columns[29])
```

```{python}
from sklearn.preprocessing import StandardScaler

X_test = df_test.drop((['rating']), axis = 1)
y_test = df_test['rating']

sc = StandardScaler()
X_train_std = sc.fit_transform(X_train)
X_test_std = sc.transform(X_test)
```

```{python}
#from sklearn.neighbors import KNeighborsClassifier

#model = KNeighborsClassifier(n_neighbors=10)

model.fit(X_train_std, y_train)

print('Training accuracy:', np.mean(model.predict(X_train_std) == y_train)*100)
print('Test accuracy:', np.mean(model.predict(X_test_std) == y_test)*100)
```

```{python}
from sklearn.feature_selection import SequentialFeatureSelector as SFS
from sklearn.neighbors import KNeighborsClassifier

X_train = df_train.drop((['rating']), axis = 1)
y_train = df_train['rating']

model = KNeighborsClassifier(n_neighbors=10)

# Using Forward Elimination
sfs_back = SFS(model, 
         n_features_to_select = 10,
         direction = 'backward',
         scoring = 'accuracy',
         n_jobs = -1,
         cv=10)

sfs_back = sfs_back.fit(X_train, y_train)
```

```{python}
np.arange(X_train.shape[1])[sfs_back.support_]
```

```{python}
print(X_train.columns[0])
print(X_train.columns[9])
print(X_train.columns[10])
print(X_train.columns[11])
print(X_train.columns[14])
print(X_train.columns[17])
print(X_train.columns[20])
print(X_train.columns[21])
print(X_train.columns[23])
print(X_train.columns[29])
```

```{python}
sfs_back.get_support()
```
