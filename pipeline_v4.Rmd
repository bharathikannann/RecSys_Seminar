---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.13.8
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
from sklearn import metrics
from sklearn.ensemble import RandomForestRegressor
import pandas as pd
from configparser import ConfigParser
import json
import os
from os.path import exists
import pickle
import numpy as np
from collections import OrderedDict
import matplotlib.pyplot as plt
import re

from myfm import MyFMRegressor
```

```{python}
#Read config.ini file
config = ConfigParser()
config.read("config.ini")
dataset_info = config["DATASETS"]
fm_data_path = dataset_info['fm_path']
fm_data_path_new = dataset_info['fm_path_new']
rf_data_path = dataset_info['rf_path']
train_set_filename = dataset_info['train_filename']
test_set_filename = dataset_info['test_filename']
description_filename = dataset_info['description_filename']

model_info = config["MODELS"]
model_path = model_info['model_path']
fm_filename = model_info['fm_model_filename']
rf_filename = model_info['rf_model_filename']

fs_info = config['FEATURE_SELECTION']
fs_path = fs_info['fs_rf_path']
fs_final_path = fs_info['fs_final_path']
```

### FM Training and Testing

```{python}
train = pickle.load(open(fm_data_path_new + train_set_filename, 'rb'))
test = pickle.load(open(fm_data_path_new + test_set_filename, 'rb'))
X_train = train.drop(['rating'], axis=1)
X_test = test.drop(['rating'], axis=1)
y_train = train['rating']
y_test = test['rating']


with open(fm_data_path + description_filename) as f:
    meta_data = json.load(f)
    group_shapes = [v for k,v in meta_data.items()]
```

# Baseline Model

```{python}
fm = MyFMRegressor(rank=10, random_seed = 332)
fm.fit(X_train, y_train, n_iter=200, n_kept_samples=200)

prediction = fm.predict(X_test)
rmse = ((y_test - prediction) ** 2).mean() ** .5
mae = np.abs(y_test - prediction).mean()
print(f'rmse={rmse}, mae={mae}')
```

---


# Random Forest column names to MyFM columns

```{python}
# Getting all the methods we have implemeted, file name represents the method name we have immplemented
available_methods_rf = []
for subdir, dirs, files in os.walk(fs_final_path):
    if len(dirs) == 1:
        for file in files:
            available_methods_rf.append(file[:-5])
pd.DataFrame(available_methods_rf)
```

```{python}
# Combining all the data we have in our feature selection folder for randomforest
# Folder names represent the feature seletion method, so creating a key for all methods
selected_col_dict_rf = {}
for i in available_methods_rf:
    selected_col_dict_rf[i] = []
selected_col_dict_rf.keys()
```

```{python}
# Storing corresponding json data in the key.
for i in range(len(available_methods_rf)):
    f = open(fs_final_path + available_methods_rf[i] + ".json")
    selected_col_dict_rf[available_methods_rf[i]] = json.load(f)
```

```{python}
path = dataset_info['ori_path']
genre_cols = pd.read_csv(path + 'ml-100k/u.genre', sep='|', header=None)[0].to_numpy().tolist()
```

```{python}
# # choose the method
# choose_method = 4
# sel_from_model_col_rf = selected_col_dict_rf[available_methods_rf[choose_method]]["columns"].copy()
# available_methods_rf[choose_method]
```

# Training MyFM model


## Select from model

```{python}
path = 'feature_selection/final/f-classif/'

# Getting all the methods we have implemeted, file name represents the method name we have immplemented
available_methods_sfm = []
for subdir, dirs, files in os.walk(path):
    for file in files:
        available_methods_sfm.append(file[:-5])
pd.DataFrame(available_methods_sfm)
```

```{python}
# Storing corresponding json data in the key.
select_from_model_dict = {}
for i in range(len(available_methods_sfm)):
    f = open(path + available_methods_sfm[i] + ".json")
    select_from_model_dict[available_methods_sfm[i]] = json.load(f)
```

```{python}
for i in range(1,len(select_from_model_dict)):
    temp = "f-classif_" + str(i*50)
    select_from_model_dict[temp]['columns'][::-1]
```

```{python}
def covert_fs_to_fm(cols, X_fm, genre_cols):
    
    '''
    Convert the columns obtained from rf dataset methods to fm dataset
    * cols - all the selected columns
    * X_fm - all the myfm data
    * genre_cols - The total genre list. Used to group them in group shapes.
                   Every genre is a different feaure in random forest. but in MyFM 
                   all the genre should be combined into a single key.
    
    '''
    
    # --------------------------------------------------------
    # Preprocessing columns
    # -------------------------------------------X_train_temp_fs-------------
    group_shapes = OrderedDict()
     
    X_train_temp_fs = pd.concat([X_fm.filter(regex = 'user_id'), X_fm.filter(regex = 'movie_id')], axis = 1)
    group_shapes['user_id'] = 943
    group_shapes['movie_id_id'] = 1681
    
    X_temp = X_fm[cols]
    X_temp_cols = X_temp.columns
    
    
    for i in ['timestamp','release_date','age','sex','occupation','zip_code']:
        if len(X_temp.filter(regex = i).columns):
            X_train_temp_fs = pd.concat([X_train_temp_fs, X_temp.filter(regex = i)], axis = 1)
            group_shapes[i] = len(X_temp.filter(regex = i).columns)

    if list(set(X_temp_cols) & set(genre_cols)):
        group_shapes['genre'] = 0
        for col in list(set(X_temp_cols) & set(genre_cols)):
            temp = X_temp.filter(regex=col, axis=1)
            X_train_temp_fs = pd.concat([X_train_temp_fs, temp], axis = 1)
            group_shapes['genre'] = group_shapes['genre'] + 1 
    
    group_shapes_list = [j for i,j in group_shapes.items()]
    return X_train_temp_fs, group_shapes, group_shapes_list
```

```{python}
sel_from_model_col_rf = select_from_model_dict["f-classif_" + str(8*50)]['columns'][::-1]
X_train_t, group_shapes_dict, group_shapes = covert_fs_to_fm(sel_from_model_col_rf, X_train, genre_cols)
X_train_t.shape
```

```{python}
rmse_results = []

X_train_temp = pd.concat([X_train.filter(regex = 'user_id'), X_train.filter(regex = 'movie_id')], axis = 1)
X_test_temp = pd.concat([X_test.filter(regex = 'user_id'), X_test.filter(regex = 'movie_id')], axis = 1)

for i in range(10,11):

    sel_from_model_col_rf = select_from_model_dict["f-classif_" + str(i*50)]['columns'][::-1]
    
    X_train_final, group_shapes_dict, group_shapes = covert_fs_to_fm(sel_from_model_col_rf, X_train, genre_cols)
    X_test_final, group_shapes_dict, group_shapes = covert_fs_to_fm(sel_from_model_col_rf, X_test, genre_cols)

    print(X_train_final.shape)
    fm = MyFMRegressor(rank=10, random_seed = 332)
    fm.fit(X_train_final, y_train, n_iter=200, n_kept_samples=200, group_shapes = group_shapes)

    prediction = fm.predict(X_test_final)
    rmse = ((y_test - prediction) ** 2).mean() ** .5
    mae = np.abs(y_test - prediction).mean()
    print(f'rmse={rmse}, mae={mae}')
    rmse_results.append(rmse)
    del(fm)
```

## Tree based methods

```{python}
# choose the method
choose_method = 0
sel_from_model_col_rf = selected_col_dict_rf[available_methods_rf[choose_method]]["columns"].copy()
```

```{python}
len(sel_from_model_col_rf)
```

```{python}
available_methods_rf[choose_method]
```

```{python}
rmse_results = []

# choose the method
choose_method = 2
sel_from_model_col_rf = selected_col_dict_rf[available_methods_rf[choose_method]]["columns"].copy()
print(available_methods_rf[choose_method])

X_train_temp = pd.concat([X_train.filter(regex = 'user_id'), X_train.filter(regex = 'movie_id')], axis = 1)
X_test_temp = pd.concat([X_test.filter(regex = 'user_id'), X_test.filter(regex = 'movie_id')], axis = 1)

for k in [len(sel_from_model_col_rf)]:

    X_train_final, group_shapes_dict, group_shapes = covert_fs_to_fm(sel_from_model_col_rf[:k], X_train, genre_cols)
    X_test_final, group_shapes_dict, group_shapes = covert_fs_to_fm(sel_from_model_col_rf[:k], X_test, genre_cols)

    print(X_train_final.shape)
    fm = MyFMRegressor(rank=10, random_seed = 332)
    fm.fit(X_train_final, y_train, n_iter=200, n_kept_samples=200)

    prediction = fm.predict(X_test_final)
    rmse = ((y_test - prediction) ** 2).mean() ** .5
    mae = np.abs(y_test - prediction).mean()
    print(f'rmse={rmse}, mae={mae}')
    rmse_results.append(rmse)
```

# Save and plot model

```{python}
rmse_results_copy = rmse_results.copy()
rmse_results.insert(0,0)
```

```{python}
rmse_results
```

```{python}
df = pd.read_csv("feature_selection/final_results/extra_tree_classifier_improved.csv", header = None, names=['extra_tree_classifier'])
df['permutation_importance'] = pd.read_csv("feature_selection/final_results/permutation_importance_improved.csv", header = None)
df["random_forest"] = pd.read_csv("feature_selection/final_results/random_forest_improved.csv", header = None)
df["select_from_model"] = pd.read_csv("feature_selection/final_results/select_from_model_improved.csv", header = None)
df["classif"] = pd.read_csv("feature_selection/final_results/f-classif.csv", header = None)
df["chi2"] = pd.read_csv("feature_selection/final_results/chi2.csv", header = None)
df["mutual_importance_regression"] = pd.read_csv("feature_selection/final_results/mutual_importance_regression.csv", header = None)
df["recursive_feature_elimination"] = pd.read_csv("feature_selection/final_results/recursive_feature_elimination.csv", header = None)
```

```{python}
colors = ['olive','b','c','m','y','brown','r','g']
plt.figure(figsize=(12,6))
plt.ylim(0.85,0.93)
plt.xlim(550,-50)
for c, i in enumerate(range(len(df.columns))):
    plt.plot([0,50,100,150,200,250,300,350,400,450,500,524], df.iloc[:,i].values[1:], color = colors[i], label = df.iloc[:,i].name, linestyle = '--', marker = '*')
plt.xlabel("No of Additional Parameters")
plt.ylabel("Validation RMSE")
plt.legend(bbox_to_anchor=(1.31,1))
plt.show()
```

```{python}
np.savetxt("feature_selection/final_results/f-classif.csv", 
           rmse_results,
           delimiter =", ", 
           fmt ='% s')
```

# Baseline

```{python}
fm = MyFMRegressor(rank=10)
fm.fit(X_train, y_train, n_iter=200, group_shapes=group_shapes, n_kept_samples=200)
```

```{python}
prediction = fm.predict(X_test)
rmse = ((y_test - prediction) ** 2).mean() ** .5
mae = np.abs(y_test - prediction).mean()
print(f'rmse={rmse}, mae={mae}')
```

# Random Experiments

Just randomly selecting some columns based on some analysis and seeing how the performance improves

```{python}
import ipywidgets as widgets
w = widgets.SelectMultiple(
    options=['movie_id','user_id', 'timestamp', 'release_date', 'zip_code', 'age', 'occupation', 'sex',
       'last_watched', 'mean_rate', 'cum_mean_rate', 'unknown', 'Action',
       'Adventure', 'Animation', "Children's" ,'Comedy', 'Crime',
       'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical',
       'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'],
    value = ['movie_id','user_id','timestamp', 'release_date', 'zip_code', 'age', 'occupation', 'sex',
       'last_watched', 'mean_rate', 'cum_mean_rate'],
    rows=20,
    description='Columns',
    disabled=False
)
w
```

```{python}
random_selections = list(w.value)
random_selections
```

```{python}
# convert the dateset
X_train_random_selection, meta_data_random_selection= covert_rf_to_fm(random_selections, X_train, genre_cols)
X_test_random_selection, meta_data_random_selection= covert_rf_to_fm(random_selections, X_test, genre_cols)

# col_to_drop = 5
# meta_data_random_selection['occupation'] = col_to_drop

# col_to_drop = 5
# meta_data_random_selection['zip_code'] = col_to_drop
# Convert metadata to list
meta_data_random_selection_list = [j for i,j in meta_data_random_selection.items()]

meta_data_random_selection
```

```{python}
# f = open(fs_path + 'random-forest-feature-importance_v2'  + ".json")
# temp = json.load(f)

# import re
# r = re.compile(".*zip_code")
# newlist = list(filter(r.match, temp['columns'])) 
# newlist[5:]

# X_train_random_selection.drop(newlist[col_to_drop:], axis=1, inplace = True)
# X_test_random_selection.drop(newlist[col_to_drop:], axis=1, inplace = True)
```

```{python}
fm = MyFMRegressor(rank=10)
fm.fit(X_train_random_selection, y_train, n_iter=200, group_shapes=meta_data_random_selection_list, n_kept_samples=200)

# # Training data RMSE
# prediction_train = fm.predict(X_train_random_selection)
# rmse_train = ((y_train - prediction_train) ** 2).mean() ** .5
# mae_train = np.abs(y_train - prediction_train).mean()
# print("Training Metrics")
# print(f'rmse={rmse_train}, mae={mae_train}')

# Test data RMSE
prediction = fm.predict(X_test_random_selection)
rmse = ((y_test - prediction) ** 2).mean() ** .5
mae = np.abs(y_test - prediction).mean()
print("Test Metrics")
print(f'rmse={rmse}, mae={mae}')
```

```{python}
random_selections_info = {}
random_selections_info["Columns"] = ', '.join(random_selections)
random_selections_info['rmse_train'] = str(rmse_train)
random_selections_info['mse_train'] = str(mae_train)
random_selections_info['rmse_test'] = str(rmse)
random_selections_info['mse_test'] = str(mae)
random_selections_info['Additional Info'] = ""

import json
with open("rmse_infos/" + str(round(rmse,6))+".json", "w") as outfile:
    json.dump(random_selections_info, outfile)
```

# Random Forest Model for MYFM data

```{python}
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
```

```{python}
# initialize and fit the model
forest = RandomForestClassifier(n_estimators=100)
forest.fit(X_train_random_selection, y_train)
```

```{python}
from sklearn.metrics import accuracy_score, confusion_matrix
y_pred = forest.predict(X_test_random_selection)
accuracy_score(y_test, y_pred) * 100
# confusion_matrix(y_test, y_pred)
```

```{python}
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
```

```{python}
# pickle.dump(forest, open('rmse_infos/randomforestforfmdata', 'wb'))
```

```{python}
# list of column names
feature_names = list(X_train_random_selection.columns)

# extract the feature importance values
std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)
rf_feature_importances = pd.DataFrame(
    {"feature": feature_names, "importance": forest.feature_importances_}
)

rf_feature_importances.sort_values("importance", ascending=False,inplace=True)
```

```{python}
rf_feature_importances
```

```{python}
import matplotlib.pyplot as plt
# Visualization is nice to have, but not necessary
# visualize the importance of each feature
fig, ax = plt.subplots(figsize=(12,6))
# ax.set_ylim([0, 30])
rf_feature_importances.plot.bar(x='feature', y='importance', ax=ax, legend=False, stacked = True)
ax.set_title("Feature importances")
ax.set_ylabel("Importance in %")
fig.tight_layout()
```

# Default program given my MyFM

```{python}
# import numpy as np
# from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder
# from sklearn import metrics

# import myfm
# from myfm.utils.benchmark_data import MovieLens100kDataManager

# FM_RANK = 10

# data_manager = MovieLens100kDataManager()
# df_train, df_test = data_manager.load_rating_predefined_split(fold=3)

# FEATURE_COLUMNS = ['user_id', 'movie_id']
# ohe = OneHotEncoder(handle_unknown='ignore')

# X_train = ohe.fit_transform(df_train[FEATURE_COLUMNS])
# X_test = ohe.transform(df_test[FEATURE_COLUMNS])
# y_train = df_train.rating.values
# y_test = df_test.rating.values

# fm = myfm.MyFMRegressor(rank=FM_RANK, random_seed=42)
# fm.fit(X_train, y_train, n_iter=200, n_kept_samples=200)

# prediction = fm.predict(X_test)
# rmse = ((y_test - prediction) ** 2).mean() ** .5
# mae = np.abs(y_test - prediction).mean()
# print(f'rmse={rmse}, mae={mae}')
```

---

```{python}
# callback = MyRegressionCallback(5, X_test, y_test.values)

# create parent folder if doesn't exist
os.makedirs(model_path, exist_ok=True)

# load from pickle dump, if it exists. Otherwise train model and then save/'pickle' it
fm_model_path = model_path + fm_filename
if exists(fm_model_path):
    fm = pickle.load(open(fm_model_path, 'rb'))
else:
    fm = MyFMRegressor(rank=1).fit(X_train, y_train, n_iter=300, group_shapes=group_shapes)
    pickle.dump(fm, open(fm_model_path, 'wb'))

fm_error = metrics.mean_squared_error(y_test, fm.predict(X_test), squared=False)
print(f'FM Regression error: {fm_error}')
```

### RF Training and Testing

```{python}
train = pickle.load(open(rf_data_path + train_set_filename, 'rb'))
test = pickle.load(open(rf_data_path + test_set_filename, 'rb'))
# train = pd.read_csv(rf_data_path + train_set_filename, sep=',', encoding='latin-1', index_col=None, nrows=1000)
# test = pd.read_csv(rf_data_path + test_set_filename, sep=',', encoding='latin-1', index_col=None, nrows=1000)
X_train = train.drop(['rating'], axis=1)
X_test = test.drop(['rating'], axis=1)
y_train = train['rating']
y_test = test['rating']
```

```{python}
rf_model_path = model_path + rf_filename
if exists(rf_model_path):
    rf = pickle.load(open(rf_model_path, 'rb'))
else:
    rf = RandomForestRegressor(n_estimators = 100, random_state = 42).fit(X_train, y_train)
    pickle.dump(rf, open(rf_model_path, 'wb'))

rf_error = metrics.mean_squared_error(y_test, rf.predict(X_test), squared=False)
print(f'Random Forest error: {rf_error}')

```

### RF Feature Importance

```{python}
import matplotlib.pyplot as plt

# list of column names
feature_names = list(X_train.columns)

# extract the feature importance values
std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)
rf_feature_importances = pd.DataFrame(
    {"feature": feature_names, "importance": rf.feature_importances_}
)

rf_feature_importances.sort_values("importance", ascending=False,inplace=True)

# visualize the importance of each feature
fig, ax = plt.subplots(figsize=(12,6))
rf_feature_importances.plot.bar(x='feature', y='importance', yerr=std, ax=ax, legend=False)
ax.set_title("Feature importances")
ax.set_ylabel("Mean decrease in impurity")
fig.tight_layout()
```
