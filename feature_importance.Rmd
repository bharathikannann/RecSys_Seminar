---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.13.8
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
# import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import time
from configparser import ConfigParser
import os
import json

from sklearn.metrics import accuracy_score, confusion_matrix
```

```{python}
#Read config.ini file
config = ConfigParser()
config.read("config.ini")
dataset_info = config["DATASETS"]
fm_data_path = dataset_info['fm_path']
rf_data_path = dataset_info['rf_path']
train_set_filename = dataset_info['train_filename']
test_set_filename = dataset_info['test_filename']
description_filename = dataset_info['description_filename']

model_info = config["MODELS"]
model_path = model_info['model_path']
fm_filename = model_info['fm_model_filename']
rf_filename = model_info['rf_model_filename']

fs_info = config["FEATURE_SELECTION"]
fs_fm_path = fs_info['fs_fm_path']
fs_rf_path = fs_info['fs_rf_path']
```

> **_Important:_**  Create a new heading for the model, and also be unique with variable names.  

### NOTES:
- Save final outputs in this format: 
    
```python
    x_final_dict = {
        "k": None,
        "columns": [], # list type
        "importances": [], # list type, in percentage, and should be sorted
    }
```
- x - your model name
- columns length must be same as importances length
- importances must be in descending order, and columns should follow accordingly
- Remember to exclude all movie_id and user_id columns before doing feature selection (optional and experiment with this one)

### Then follow this format to save dictionary to json

- Follow filename format (pay attention to - and _): model-name_k.json
    - k - if you have any k values or just model_name

```{python}
# create parent folder if doesn't exist
os.makedirs(fs_rf_path, exist_ok=True)
```

```{python}
train = pickle.load(open(rf_data_path + train_set_filename, 'rb'))
test = pickle.load(open(rf_data_path + test_set_filename, 'rb'))
```

## Training data

```{python}
X = train.drop((['rating', 'movie_id', 'user_id']), axis = 1)
y = train['rating']
```

```{python}
# Save for later reference, can be removed after all experiments
X_original = X.copy(deep = True)
```

## Test data

```{python}
X_test = test.drop((['rating', 'movie_id', 'user_id']), axis = 1)
y_test = test['rating']
```

Experimenting with one hot encoding occupation to see if performance increases. 
It was the same and this code is just for reference. Will be removed at the end.

```{python}
# X_ohe_occupation = X.copy(deep=True)
# dummies = pd.get_dummies(X_ohe_occupation['occupation'])
# X_ohe_occupation.drop('occupation', axis = 1, inplace=True)
# X_ohe_occupation = pd.concat([X_ohe_occupation, dummies], axis = 1)
```

```{python}
# X_test_ohe_occupation = X_test.copy(deep=True)
# dummies = pd.get_dummies(X_test_ohe_occupation['occupation'])
# X_test_ohe_occupation.drop('occupation', axis = 1, inplace=True)
# X_test_ohe_occupation = pd.concat([X_test_ohe_occupation, dummies], axis = 1)
```

---
# 1. Feature Importance
# 1.1 Feature importance using Random Forest

```{python}
from sklearn.ensemble import RandomForestClassifier
```

```{python}
# initialize and fit the model
forest = RandomForestClassifier(n_estimators=100)
forest.fit(X, y)
```

### Testing the model

```{python}
y_pred = forest.predict(X_test)
accuracy_score(y_test, y_pred) * 100
# confusion_matrix(y_test, y_pred)
```

```{python}
# list of column names
feature_names = list(X.columns)

# extract the feature importance values
std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)
rf_feature_importances = pd.DataFrame(
    {"feature": feature_names, "importance": forest.feature_importances_}
)
```

```{python}
# giving colors for all genre for better visualization, optional
# my_colors = ['blue','green']
# my_colors.extend(['orange']*19)
# my_colors.extend(['brown','pink','gray','olive','cyan','aquamarine','gold','gold'])
# feature_importances_df['info'] = my_colors
```

```{python}
rf_feature_importances.sort_values("importance", ascending=False,inplace=True)
```

```{python}
# Converting the importances to percentage
# rf_feature_importances['importance'] = rf_feature_importances['importance'] * 100
```

```{python}
rf_final_dict = {
    "k": None,
    "columns": rf_feature_importances['feature'].tolist(),
    "importances": rf_feature_importances['importance'].tolist(),
}
```

```{python}
with open(fs_rf_path + "random-forest-feature-importance.json", "w") as fp:
    json.dump(rf_final_dict, fp) 
```

```{python}
# Visualization is nice to have, but not necessary
# visualize the importance of each feature
fig, ax = plt.subplots(figsize=(12,6))
# ax.set_ylim([0, 30])
rf_feature_importances.plot.bar(x='feature', y='importance', ax=ax, legend=False, stacked = True)
ax.set_title("Feature importances")
ax.set_ylabel("Importance in %")
fig.tight_layout()
```

---

# 1. 2. Extra Tree Cassifier

```{python}
from sklearn.ensemble import ExtraTreesClassifier
```
```{python}
extra_tree = ExtraTreesClassifier()
extra_tree.fit(X,y)
```

```{python}
# extract the feature importance values
std = np.std([tree.feature_importances_ for tree in extra_tree.estimators_], axis=0)
et_feature_importances = pd.DataFrame(
    {"feature": feature_names, "importance": extra_tree.feature_importances_}
)
```

```{python}
et_feature_importances.sort_values("importance", ascending=False,inplace=True)
```

```{python}
# visualize the importance of each feature
fig, ax = plt.subplots(figsize=(12,6))
et_feature_importances.plot.bar(x='feature', y='importance', yerr=std, ax=ax, legend=False)
ax.set_title("Feature importances")
ax.set_ylabel("Mean decrease in impurity")
fig.tight_layout()
```

```{python}
# save the info
et_final_dict = {
    "k": None,
    "columns": et_feature_importances['feature'].tolist(),
    "importances": et_feature_importances['importance'].tolist(),
}

with open(fs_rf_path + "extra-tree-classifier-feature-importance.json", "w") as fp:
    json.dump(et_final_dict, fp) 
```

Visualizing top 10 common features for random forest and extra trees

```{python}
common_imp_features = pd.merge(rf_feature_importances.iloc[:11], et_feature_importances.iloc[:11], how = 'inner', on = ['feature'])
common_imp_features.rename(columns={'importance_x':'random_forest_importance','importance_y':'extra_tree_importance'}, inplace=True)
```

```{python}
# visualize the importance of each feature
common_imp_features.plot.bar(x='feature', y=['random_forest_importance','extra_tree_importance'], legend=True)
```

both trees almost gave the same important features. These are the top 10 important features for our dataset.


___
# 2. Permutation Importance

```{python}
from sklearn.inspection import permutation_importance
start_time = time.time()
result = permutation_importance(
    forest, X, y, n_repeats=4
)
elapsed_time = time.time() - start_time
print(f"Elapsed time to compute the importances: {elapsed_time:.3f} seconds")

forest_importances = pd.Series(result.importances_mean, index=feature_names)
forest_importances.sort_values(ascending=False,inplace=True)
```

```{python}
# save the info
et_final_dict = {
    "k": None,
    "columns": forest_importances.index.tolist(),
    "importances": forest_importances.values.tolist(),
}

with open(fs_rf_path + "permutation_importance.json", "w") as fp:
    json.dump(et_final_dict, fp) 
```

```{python}
fig, ax = plt.subplots(figsize=(12,6))
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
```

---
# Analysis - Correlation matrix with heatmap

```{python}
#get correlations of each features in dataset and plot heatmap
corrmat = pd.concat([X,y], axis = 1).corr()
top_corr_features = corrmat.index
plt.figure(figsize=(24,24))
g=sns.heatmap(pd.concat([X,y], axis = 1)[top_corr_features].corr(),annot=True,cmap="RdYlGn")
plt.tight_layout()
```

```{python}
plt.figure(figsize=(24,10))
g2=sns.heatmap(pd.DataFrame(pd.concat([X,y], axis = 1)[top_corr_features].corr().loc['rating']).T,annot=False,cmap="RdYlGn")
```

- This heatmap is to show which features are highly correlated with the output feature
- positive - increase in one value increases the target value and viceversa
- cummulative mean rating and and mean rating have high correlation with the rating. 
- this is because these features are extracted from rating. 
- movie id and release date don't have high correlation with the rating, but still those features are important. 


# 3. Embedded methods

```{python}
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso, LogisticRegression
from sklearn.feature_selection import SelectFromModel
from sklearn.preprocessing import StandardScaler
```

```{python}
scaler = StandardScaler()
scaled = scaler.fit_transform(X)
```

```{python}
sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l2'))
sel_.fit(scaled, y)
```

```{python}
sel_.get_support()
```

```{python}
selected_feat = X.columns[(sel_.get_support())]
print('total no of features: {}'.format((X.shape[1])))
print('No of selected features: {}'.format(len(selected_feat)))
print('features with coefficients shrank to zero: {}'.format(
      np.sum(sel_.estimator_.coef_ == 0)))
```

```{python}
# save the info
et_final_dict = {
    "k": None,
    "columns": selected_feat.tolist(),
    "importances": None,
}

with open(fs_rf_path + "select_from_model.json", "w") as fp:
    json.dump(et_final_dict, fp) 
```

# Van Methods


### Feature selection with Mutual Information Regression method

```{python}
from sklearn.feature_selection import mutual_info_regression
```

```{python}
mutual_info = mutual_info_regression(X, y)
mutual_info
```

```{python}
mutual_info = pd.Series(mutual_info)
mutual_info.index = X.columns
mutual_info.sort_values(ascending=False)
```

```{python}
mutual_info.sort_values(ascending=False).plot.bar(figsize=(15,5))
```

```{python}
mutual_info_sort = mutual_info.sort_values(ascending=False)
```

```{python}
# save the info
mi_final_dict = {
    "k": None,
    "columns": mutual_info_sort.index.tolist(),
    "importances": mutual_info_sort.values.tolist(),
}

with open(fs_rf_path + "mutual_importance_regression.json", "w") as fp:
    json.dump(mi_final_dict, fp) 
```

### Feature selection with F regression method

```{python}
from sklearn.datasets import make_regression
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression
```

```{python}
# feature selection function
def select_features(X, y, X_test):
    # configure to select all features
    fs = SelectKBest(score_func=f_regression, k='all')
    # learn relationship from training data
    fs.fit(X, y)
    # transform train input data
    X_fs = fs.transform(X)
    # transform test input data
    X_test_fs = fs.transform(X_test)
    return X_fs, X_test_fs, fs
```

```{python}
# feature selection
X_fs, X_test_fs, fs = select_features(X, y, X_test)

# what are scores for the features
for i in range(len(fs.scores_)):
    print('Feature %d: %f' % (i, fs.scores_[i]))
    
# plot the scores
plt.bar([i for i in range(len(fs.scores_))], fs.scores_)
plt.show()
```

```{python}
fs.scores_
fs.scores_ = pd.Series(fs.scores_)
fs.scores_.index = X.columns
fs.scores_.sort_values(ascending=False)
```

```{python}
fs.scores_.sort_values(ascending=False).plot.bar(figsize=(15,5))
```

```{python}
fs.scores_sort = fs.scores_.sort_values(ascending=False)
```

```{python}
# save the info
fr_final_dict = {
    "k": None,
    "columns": fs.scores_sort.index.tolist(),
    "importances": fs.scores_sort.values.tolist(),
}

with open(fs_rf_path + "f_regression.json", "w") as fp:
    json.dump(fr_final_dict, fp) 
```

### Feature selection with Sequential Feature Selection method


### 1. Forward Elimination method

```{python}
from sklearn.feature_selection import SequentialFeatureSelector as SFS
from sklearn.neighbors import KNeighborsClassifier
```

```{python}
model = KNeighborsClassifier(n_neighbors=10)

# Using Forward Elimination
sfs = SFS(model, 
         n_features_to_select = 10,
         direction = 'forward',
         scoring = 'accuracy',
         n_jobs = -1,
         cv=5)

sfs = sfs.fit(X, y)
```

```{python}
np.arange(X.shape[1])[sfs.support_]
```

```{python}
selected_feat_fw = X.columns[(sfs.support_)]
selected_feat_fw
```

```{python}
# save the info
fw_final_dict = {
    "k": 10,
    "columns": selected_feat_fw.tolist(),
    "importances": None,
}

with open(fs_rf_path + "forward_elimination.json", "w") as fp:
    json.dump(fw_final_dict, fp) 
```

### 2. Backward Elimination method

```{python}
from sklearn.preprocessing import StandardScaler
```

```{python}
sc = StandardScaler()
X_std = sc.fit_transform(X)
X_test_std = sc.transform(X_test)
```

```{python}
model.fit(X_std, y)

print('Training accuracy:', np.mean(model.predict(X_std) == y)*100)
print('Test accuracy:', np.mean(model.predict(X_test_std) == y_test)*100)
```

```{python}
X_test.head()
```

```{python}
# Using backward Elimination
sfs_backward = SFS(model, 
         n_features_to_select = 10,
         direction = 'backward',
         scoring = 'accuracy',
         n_jobs = -1,
         cv=5)

sfs_backward = sfs_backward.fit(X, y)
```

# Fadi Methods

```{python}

```

---

```{python}

```
